#! /usr/bin/env python

import logging
import os
import re
import sys
from asyncio import gather, run, sleep
from asyncio.subprocess import create_subprocess_exec
from pathlib import Path
from shutil import which
from subprocess import check_call
from typing import Any, Dict, List, Optional, Set, Tuple
from zipfile import ZipFile

from aiohttp import ClientSession
from yaml import safe_load

logger = logging.getLogger(__name__)
log = logger.info


async def async_exec(
    command: str, *arguments: str, env: Optional[Dict[str, str]] = None
) -> None:
    process = await create_subprocess_exec(command, *arguments, env=env)
    return_code = await process.wait()
    if return_code != 0:
        raise RuntimeError(
            f"Failed to execute {command} {arguments} (return code: {return_code})"
        )


# Post a comment on a given issue/PR with text in message
async def send_comment(session: ClientSession, issue_number: int, message: str) -> None:
    token = os.environ["BOT_TOKEN"]
    url = (
        f"https://api.github.com/repos/bioconda/bioconda-recipes/issues/{issue_number}/comments"
    )
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    payload = {"body": message}
    log("Sending comment: url=%s", url)
    log("Sending comment: payload=%s", payload)
    async with session.post(url, headers=headers, json=payload) as response:
        status_code = response.status
        log("the response code was %d", status_code)
        if status_code < 200 or status_code > 202:
            sys.exit(1)


def list_zip_contents(fname: str) -> [str]:
    f = ZipFile(fname)
    return [e.filename for e in f.infolist() if e.filename.endswith('.tar.gz') or e.filename.endswith('.tar.bz2')]


# Download a zip file from url to zipName.zip and return that path
# Timeout is 30 minutes to compensate for any network issues
async def download_file(session: ClientSession, zipName: str, url: str) -> str:
    async with session.get(url, timeout=60*30) as response:
        if response.status == 200:
            ofile = f"{zipName}.zip"
            with open(ofile, 'wb') as fd:
                while True:
                    chunk = await response.content.read(1024*1024*1024)
                    if not chunk:
                        break
                    fd.write(chunk)
            return ofile
    return None


# Find artifact zip files, download them and return their URLs and contents
async def fetch_azure_zip_files(session: ClientSession, buildId: str) -> [(str, str)]:
    artifacts = []

    url = f"https://dev.azure.com/bioconda/bioconda-recipes/_apis/build/builds/{buildId}/artifacts?api-version=4.1"
    log("contacting azure %s", url)
    async with session.get(url) as response:
        # Sometimes we get a 301 error, so there are no longer artifacts available
        if response.status == 301:
            return artifacts
        res = await response.text()

    res_object = safe_load(res)
    if res_object['count'] == 0:
        return artifacts

    for artifact in res_object['value']:
        zipName = artifact['name']  # LinuxArtifacts or OSXArtifacts
        zipUrl = artifact['resource']['downloadUrl']
        log(f"zip name is {zipName} url {zipUrl}")
        fname = await download_file(session, zipName, zipUrl)
        if not fname:
            continue
        pkgsImages = list_zip_contents(fname)
        for pkg in pkgsImages:
            artifacts.append((zipUrl, pkg))

    return artifacts


def parse_azure_build_id(url: str) -> str:
    return re.search("buildId=(\d+)", url).group(1)


# Given a PR and commit sha, fetch a list of the artifact zip files URLs and their contents
async def fetch_pr_sha_artifacts(session: ClientSession, pr: int, sha: str) -> List[Tuple[str, str]]:
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/commits/{sha}/check-runs"

    headers = {
        "User-Agent": "BiocondaCommentResponder",
        "Accept": "application/vnd.github.antiope-preview+json",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    check_runs = safe_load(res)
    log(f"DEBUG url was {url} returned {check_runs}")

    for check_run in check_runs["check_runs"]:
        # The names are "bioconda.bioconda-recipes (test_osx test_osx)" or similar
        if check_run["name"].startswith("bioconda.bioconda-recipes (test_"):
            # The azure build ID is in the details_url as buildId=\d+
            buildID = parse_azure_build_id(check_run["details_url"])
            log(f"DEBUG buildID is {buildID}")
            zipFiles = await fetch_azure_zip_files(session, buildID)
            log(f"DEBUG zipFiles are {zipFiles}")
            return zipFiles  # We've already fetched all possible artifacts

    return []


# Given a PR and commit sha, post a comment with any artifacts
async def make_artifact_comment(session: ClientSession, pr: int, sha: str) -> None:
    artifacts = await fetch_pr_sha_artifacts(session, pr, sha)
    nPackages = len(artifacts)
    log(f"DEBUG the artifacts are {artifacts}")

    if nPackages > 0:
        comment = "Package(s) built on Azure are ready for inspection:\n\n"
        comment += "Arch | Package | Zip File\n-----|---------|---------\n"
        install_noarch = ""
        install_linux = ""
        install_osx = ""

        # Table of packages and repodata.json
        for URL, artifact in artifacts:
            if not (package_match := re.match(r"^((.+)\/(.+)\/(.+)\/(.+\.tar\.bz2))$", artifact)):
                continue
            url, archdir, basedir, subdir, packageName = package_match.groups()
            urlBase = URL[:-3]  # trim off zip from format=
            urlBase += "file&subPath=%2F{}".format("%2F".join([basedir, subdir]))
            conda_install_url = urlBase
            # N.B., the zip file URL is nearly identical to the URL for the individual member files. It's unclear if there's an API for getting the correct URL to the files themselves
            #pkgUrl = "%2F".join([urlBase, packageName])
            #repoUrl = "%2F".join([urlBase, "current_repodata.json"])
            #resp = await session.get(repoUrl)

            if subdir == "noarch":
                comment += "noarch |"
            elif subdir == "linux-64":
                comment += "linux-64 |"
            else:
                comment += "osx-64 |"
            comment += f" {packageName} | [{archdir}]({URL})\n"

        # Conda install examples
        comment += "***\n\nYou may also use `conda` to install these after downloading and extracting the appropriate zip file. From the LinuxArtifacts or OSXArtifacts directories:\n\n"
        comment += "```conda install -c ./packages <package name>\n```\n"

        # Table of containers
        comment += "***\n\nDocker image(s) built (images are in the LinuxArtifacts zip file above):\n\n"
        comment += "Package | Tag | Install with `docker`\n"
        comment += "--------|-----|----------------------\n"

        for URL, artifact in artifacts:
            if artifact.endswith(".tar.gz"):
                image_name = artifact.split("/").pop()[: -len(".tar.gz")]
                if ':' in image_name:
                    package_name, tag = image_name.split(':', 1)
                    #image_url = URL[:-3]  # trim off zip from format=
                    #image_url += "file&subPath=%2F{}.tar.gz".format("%2F".join(["images", '%3A'.join([package_name, tag])]))
                    comment += f"[{package_name}] | {tag} | "
                    comment += f'<details><summary>show</summary>`gzip -dc LinuxArtifacts/images/{image_name}.tar.gz \\| docker load`\n'
        comment += "\n\n"
    else:
        comment = (
            "No artifacts found on the most recent Azure build. "
            "Either the build failed, the artifacts have were removed due to age, or the recipe was blacklisted/skipped."
        )
    await send_comment(session, pr, comment)


# Post a comment on a given PR with its CircleCI artifacts
async def artifact_checker(session: ClientSession, issue_number: int) -> None:
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{issue_number}"
    headers = {
        "User-Agent": "BiocondaCommentResponder",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    pr_info = safe_load(res)

    await make_artifact_comment(session, issue_number, pr_info["head"]["sha"])


# Return true if a user is a member of bioconda
async def is_bioconda_member(session: ClientSession, user: str) -> bool:
    token = os.environ["BOT_TOKEN"]
    url = f"https://api.github.com/orgs/bioconda/members/{user}"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    rc = 404
    async with session.get(url, headers=headers) as response:
        try:
            response.raise_for_status()
            rc = response.status
        except:
            # Do nothing, this just prevents things from crashing on 404
            pass

    return rc == 204


# Reposts a quoted message in a given issue/PR if the user isn't a bioconda member
async def comment_reposter(session: ClientSession, user: str, pr: int, message: str) -> None:
    if await is_bioconda_member(session, user):
        log("Not reposting for %s", user)
        return
    log("Reposting for %s", user)
    await send_comment(
        session,
        pr,
        f"Reposting for @{user} to enable pings (courtesy of the BiocondaBot):\n\n> {message}",
    )


# Fetch and return the JSON of a PR
# This can be run to trigger a test merge
async def get_pr_info(session: ClientSession, pr: int) -> Any:
    token = os.environ["BOT_TOKEN"]
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{pr}"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    pr_info = safe_load(res)
    return pr_info


# Update a branch from upstream master, this should be run in a try/catch
async def update_from_master_runner(session: ClientSession, pr: int) -> None:
    async def git(*args: str) -> None:
        return await async_exec("git", *args)

    # Setup git, otherwise we can't push
    await git("config", "--global", "user.email", "biocondabot@gmail.com")
    await git("config", "--global", "user.name", "BiocondaBot")

    pr_info = await get_pr_info(session, pr)
    remote_branch = pr_info["head"]["ref"]
    remote_repo = pr_info["head"]["repo"]["full_name"]

    max_depth = 2000
    # Clone
    await git(
        "clone",
        f"--depth={max_depth}",
        f"--branch={remote_branch}",
        f"git@github.com:{remote_repo}.git",
        "bioconda-recipes",
    )

    async def git_c(*args: str) -> None:
        return await git("-C", "bioconda-recipes", *args)

    # Add/pull upstream
    await git_c("remote", "add", "upstream", "https://github.com/bioconda/bioconda-recipes")
    await git_c("fetch", f"--depth={max_depth}", "upstream", "master")

    # Merge
    await git_c("merge", "upstream/master")

    await git_c("push")


# Merge the upstream master branch into a PR branch, leave a message on error
async def update_from_master(session: ClientSession, pr: int) -> None:
    try:
        await update_from_master_runner(session, pr)
    except Exception as e:
        await send_comment(
            session,
            pr,
            "I encountered an error updating your PR branch. You can report this to bioconda/core if you'd like.\n-The Bot",
        )
        sys.exit(1)


# Ensure there's at least one approval by a member
async def approval_review(session: ClientSession, issue_number: int) -> bool:
    token = os.environ["BOT_TOKEN"]
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{issue_number}/reviews"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    reviews = safe_load(res)

    approved_reviews = [review for review in reviews if review["state"] == "APPROVED"]
    if not approved_reviews:
        return False

    # Ensure the review author is a member
    return any(
        gather(
            *(
                is_bioconda_member(session, review["user"]["login"])
                for review in approved_reviews
            )
        )
    )


# Check the mergeable state of a PR
async def check_is_mergeable(
    session: ClientSession, issue_number: int, second_try: bool = False
) -> bool:
    token = os.environ["BOT_TOKEN"]
    # Sleep a couple of seconds to allow the background process to finish
    if second_try:
        await sleep(3)

    # PR info
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{issue_number}"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    pr_info = safe_load(res)

    # We need mergeable == true and mergeable_state == clean, an approval by a member and
    if pr_info.get("mergeable") is None and not second_try:
        return await check_is_mergeable(session, issue_number, True)
    elif (
        pr_info.get("mergeable") is None
        or not pr_info["mergeable"]
        or pr_info["mergeable_state"] != "clean"
    ):
        return False

    return await approval_review(session, issue_number)


# Ensure uploaded containers are in repos that have public visibility
async def toggle_visibility(session: ClientSession, container_repo: str) -> None:
    url = f"https://quay.io/api/v1/repository/biocontainers/{container_repo}/changevisibility"
    QUAY_OAUTH_TOKEN = os.environ["QUAY_OAUTH_TOKEN"]
    headers = {
        "Authorization": f"Bearer {QUAY_OAUTH_TOKEN}",
        "Content-Type": "application/json",
    }
    body = {"visibility": "public"}
    rc = 0
    try:
        async with session.post(url, headers=headers, json=body) as response:
            rc = response.status
    except:
        # Do nothing
        pass
    log("Trying to toggle visibility (%s) returned %d", url, rc)


# Download an artifact from CircleCI, rename and upload it
async def download_and_upload(session: ClientSession, x: str) -> None:
    basename = x.split("/").pop()
    # the tarball needs a regular name without :, the container needs pkg:tag
    image_name = basename.replace("%3A", ":").replace("\n", "").replace(".tar.gz", "")
    file_name = basename.replace("%3A", "_").replace("\n", "")

    async with session.get(x) as response:
        with open(file_name, "wb") as file:
            logged = 0
            loaded = 0
            while chunk := await response.content.read(256 * 1024):
                file.write(chunk)
                loaded += len(chunk)
                if loaded - logged >= 50 * 1024 ** 2:
                    log("Downloaded %.0f MiB: %s", max(1, loaded / 1024 ** 2), x)
                    logged = loaded
            log("Downloaded %.0f MiB: %s", max(1, loaded / 1024 ** 2), x)

    if x.endswith(".gz"):
        # Container
        log("uploading with skopeo: %s", file_name)
        # This can fail, retry with 5 second delays
        count = 0
        maxTries = 5
        success = False
        QUAY_LOGIN = os.environ["QUAY_LOGIN"]
        env = os.environ.copy()
        # TODO: Fix skopeo package to find certificates on its own.
        skopeo_path = which("skopeo")
        if not skopeo_path:
            raise RuntimeError("skopeo not found")
        env["SSL_CERT_DIR"] = str(Path(skopeo_path).parents[1].joinpath("ssl"))
        while count < maxTries:
            try:
                await async_exec(
                    "skopeo",
                    "--command-timeout",
                    "600s",
                    "copy",
                    f"docker-archive:{file_name}",
                    f"docker://quay.io/biocontainers/{image_name}",
                    "--dest-creds",
                    QUAY_LOGIN,
                    env=env,
                )
                success = True
                break
            except:
                count += 1
                if count == maxTries:
                    raise
            await sleep(5)
        if success:
            await toggle_visibility(session, basename.split("%3A")[0])
    elif x.endswith(".bz2"):
        # Package
        log("uploading package")
        ANACONDA_TOKEN = os.environ["ANACONDA_TOKEN"]
        await async_exec("anaconda", "-t", ANACONDA_TOKEN, "upload", file_name, "--force")

    log("cleaning up")
    os.remove(file_name)


# Upload artifacts to quay.io and anaconda, return the commit sha
# Only call this for mergeable PRs!
async def upload_artifacts(session: ClientSession, pr: int) -> str:
    # Get last sha
    pr_info = await get_pr_info(session, pr)
    sha: str = pr_info["head"]["sha"]

    # Fetch the artifacts
    artifacts = await fetch_pr_sha_artifacts(session, pr, sha)
    artifacts = [artifact for artifact in artifacts if artifact.endswith((".gz", ".bz2"))]
    assert artifacts

    # Download/upload Artifacts
    for artifact in artifacts:
        await download_and_upload(session, artifact)

    return sha


# Assume we have no more than 250 commits in a PR, which is probably reasonable in most cases
async def get_pr_commit_message(session: ClientSession, issue_number: int) -> str:
    token = os.environ["BOT_TOKEN"]
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{issue_number}/commits"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    async with session.get(url, headers=headers) as response:
        response.raise_for_status()
        res = await response.text()
    commits = safe_load(res)
    message = "".join(f" * {commit['commit']['message']}\n" for commit in reversed(commits))
    return message


# Merge a PR
async def merge_pr(session: ClientSession, pr: int) -> None:
    token = os.environ["BOT_TOKEN"]
    await send_comment(
        session,
        pr,
        "I will attempt to upload artifacts and merge this PR. This may take some time, please have patience.",
    )

    try:
        mergeable = await check_is_mergeable(session, pr)
        log("mergeable state of %s is %s", pr, mergeable)
        if not mergeable:
            await send_comment(session, pr, "Sorry, this PR cannot be merged at this time.")
        else:
            log("uploading artifacts")
            sha = await upload_artifacts(session, pr)
            log("artifacts uploaded")

            # Carry over last 250 commit messages
            msg = await get_pr_commit_message(session, pr)

            # Hit merge
            url = f"https://api.github.com/repos/bioconda/bioconda-recipes/pulls/{pr}/merge"
            headers = {
                "Authorization": f"token {token}",
                "User-Agent": "BiocondaCommentResponder",
            }
            payload = {
                "sha": sha,
                "commit_title": f"[ci skip] Merge PR {pr}",
                "commit_message": f"Merge PR #{pr}, commits were: \n{msg}",
                "merge_method": "squash",
            }
            log("Putting merge commit")
            async with session.put(url, headers=headers, json=payload) as response:
                rc = response.status
            log("body %s", payload)
            log("merge_pr the response code was %s", rc)
    except:
        await send_comment(
            session,
            pr,
            "I received an error uploading the build artifacts or merging the PR!",
        )
        logger.exception("Upload failed", exc_info=True)


# Add the "Please review and merge" label to a PR
async def add_pr_label(session: ClientSession, pr: int) -> None:
    token = os.environ["BOT_TOKEN"]
    url = f"https://api.github.com/repos/bioconda/bioconda-recipes/issues/{pr}/labels"
    headers = {
        "Authorization": f"token {token}",
        "User-Agent": "BiocondaCommentResponder",
    }
    payload = {"labels": ["please review & merge"]}
    async with session.post(url, headers=headers, json=payload) as response:
        response.raise_for_status()


async def gitter_message(session: ClientSession, msg: str) -> None:
    token = os.environ["GITTER_TOKEN"]
    room_id = "57f3b80cd73408ce4f2bba26"
    url = f"https://api.gitter.im/v1/rooms/{room_id}/chatMessages"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "BiocondaCommentResponder",
    }
    payload = {"text": msg}
    log("Sending request to %s", url)
    async with session.post(url, headers=headers, json=payload) as response:
        response.raise_for_status()


async def notify_ready(session: ClientSession, pr: int) -> None:
    try:
        await gitter_message(
            session,
            f"PR ready for review: https://github.com/bioconda/bioconda-recipes/pull/{pr}",
        )
    except Exception:
        logger.exception("Posting to Gitter failed", exc_info=True)
        # Do not die if we can't post to gitter!


# This requires that a JOB_CONTEXT environment variable, which is made with `toJson(github)`
async def main() -> None:
    job_context = safe_load(os.environ["JOB_CONTEXT"])
    log("%s", job_context)
    if job_context["event"]["issue"].get("pull_request") is None:
        return
    issue_number = job_context["event"]["issue"]["number"]

    original_comment = job_context["event"]["comment"]["body"]
    log("the comment is: %s", original_comment)

    comment = original_comment.lower()
    async with ClientSession() as session:
        if comment.startswith(("@bioconda-bot", "@biocondabot")):
            if "please update" in comment:
                await update_from_master(session, issue_number)
            elif " hello" in comment:
                await send_comment(session, issue_number, "Yes?")
            elif " please fetch artifacts" in comment or " please fetch artefacts" in comment:
                await artifact_checker(session, issue_number)
            elif " please merge" in comment:
                await send_comment(session, issue_number, "Sorry, I'm currently disabled")
                #await merge_pr(session, issue_number)
            elif " please add label" in comment:
                await add_pr_label(session, issue_number)
                await notify_ready(session, issue_number)
            # else:
            #    # Methods in development can go below, flanked by checking who is running them
            #      if job_context["actor"] != "dpryan79":
            #          console.log("skipping")
            #          sys.exit(0)
        elif "@bioconda/" in comment:
            await comment_reposter(
                session, job_context["actor"], issue_number, original_comment
            )


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    run(main())
